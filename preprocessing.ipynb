{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sherry975/CSC-791-Project/blob/master/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZlsmdCx4ELk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "67ab62df-eb12-41a2-c1d0-c358142894e0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# !ls \"/content/drive/My Drive/\"\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/CSC791-NLP/791 NLP project/791_project_repo\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import sys\n",
        "import traceback\n",
        "\n",
        "import re\n",
        "import datetime\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE7QYQSk4LPd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7f64ccc7-4dfe-412c-8003-3294ebb283a6"
      },
      "source": [
        "# process raw function words list and generate a txt\n",
        "\n",
        "def get_function_words_list(filename):\n",
        "    func_words_lst = []\n",
        "    with open(filename, 'r', encoding='utf-8', newline='') as f:\n",
        "        content = f.readlines()\n",
        "        for row in content:\n",
        "            if row[0] != \"#\":\n",
        "                word = row.split(\" \")[0]\n",
        "                func_words_lst.append(word)\n",
        "    return func_words_lst            \n",
        "\n",
        "\n",
        "# test\n",
        "func_words_lst = get_function_words_list(\"raw_data/james_o_shea_277.txt\")\n",
        "func_words_set = set(func_words_lst)\n",
        "\n",
        "print(len(func_words_lst), func_words_lst)\n",
        "print(len(func_words_set))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "277 ['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'be', 'became', 'because', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'both', 'but', 'by', 'can', 'cannot', 'could', 'dare', 'despite', 'did', 'do', 'does', 'done', 'down', 'during', 'each', 'eg', 'either', 'else', 'elsewhere', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'first', 'for', 'former', 'formerly', 'from', 'further', 'furthermore', 'had', 'has', 'have', 'he', 'hence', 'her', 'here', 'hereabouts', 'hereafter', 'hereby', 'herein', 'hereinafter', 'heretofore', 'hereunder', 'hereupon', 'herewith', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'i', 'ie', 'if', 'in', 'indeed', 'inside', 'instead', 'into', 'is', 'it', 'its', 'itself', 'last', 'latter', 'latterly', 'least', 'less', 'lot', 'lots', 'many', 'may', 'me', 'meanwhile', 'might', 'mine', 'more', 'moreover', 'most', 'mostly', 'much', 'must', 'my', 'myself', 'namely', 'near', 'need', 'neither', 'never', 'nevertheless', 'next', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'oftentimes', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'per', 'perhaps', 'rather', 're', 'same', 'second', 'several', 'shall', 'she', 'should', 'since', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'still', 'such', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereabouts', 'thereafter', 'thereby', 'therefore', 'therein', 'thereof', 'thereon', 'thereupon', 'these', 'they', 'third', 'this', 'those', 'though', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'under', 'until', 'up', 'upon', 'us', 'used', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'whyever', 'will', 'with', 'within', 'without', 'would', 'yes', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']\n",
            "277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zBOAuWd5zd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9097113a-83f8-48c3-a5ac-a6b554436921"
      },
      "source": [
        "# split data into train/test\n",
        "import re\n",
        "import statistics\n",
        "import random\n",
        "\n",
        "FORMALITY_MEDIAN = 4.2\n",
        "\n",
        "def clean_text(orig): \n",
        "    cleaned = orig\n",
        "    \n",
        "    if re.match('^[0-9].\\)', orig[:3]):\n",
        "        cleaned = orig[3:]\n",
        "    elif re.match('^[0-9][0-9][A-Z]' ,orig[:3]) or re.match('^[0-9].' ,orig[:2]) or re.match('^[0-9]\\)', orig[:2]):\n",
        "        cleaned = orig[2:]\n",
        "    elif re.match('^[0-9][A-Z]', orig[:2]):\n",
        "        cleaned = orig[1:]\n",
        "    \n",
        "    return cleaned\n",
        "\n",
        "def split_train_test(filename):\n",
        "    low_formality = []\n",
        "    high_formality = []\n",
        "    train_set = []\n",
        "    test_set = []\n",
        "        \n",
        "    reader = csv.reader(open(filename, newline='', encoding='mac_roman'), delimiter=',',\n",
        "                        quotechar='\"')\n",
        "    row_cnt = 0\n",
        "    \n",
        "    try:\n",
        "        for row in reader:\n",
        "            row_cnt += 1\n",
        "            \n",
        "            if \"sentence\" in row[0].lower():\n",
        "                continue\n",
        "                \n",
        "#             if row_cnt > 10:\n",
        "#                 break\n",
        "                \n",
        "            # col: 0 Sentence ID\t1 HIT ID\t2 Formality\tInformativeness\tImplicature\t5 Length in Words\t6 Length in Characters\tF-score\tI-score\tLexical Density\t11 Sentence\n",
        "            sentence = row[10]\n",
        "            formality = float(row[2])\n",
        "            cleaned_text = clean_text(sentence)\n",
        "            \n",
        "            if formality <= FORMALITY_MEDIAN:\n",
        "                # setence_id, sentence_text, len_words, len_char, formality, label\n",
        "                low_formality.append([row[0], cleaned_text, row[5], row[6], formality, \"0\"])\n",
        "            else:\n",
        "                high_formality.append([row[0], cleaned_text, row[5], row[6], formality, \"1\"])\n",
        "            \n",
        "\n",
        "        print(\"# low formality, # hgih formality: \", len(low_formality),len(high_formality))\n",
        "        \n",
        "        random.shuffle(low_formality)\n",
        "        random.shuffle(high_formality)\n",
        "        \n",
        "        num_train_low = int(len(low_formality) * 0.9)\n",
        "        num_train_high = int(len(high_formality) * 0.9)\n",
        "        train_set = low_formality[:num_train_low] + high_formality[:num_train_high]\n",
        "        test_set = low_formality[num_train_low:] + high_formality[num_train_high:]\n",
        "        \n",
        "        random.shuffle(train_set)\n",
        "        random.shuffle(test_set)\n",
        "        \n",
        "        return train_set, test_set \n",
        "\n",
        "    except Exception as ex:\n",
        "        sys.stderr.write('Exception\\n')\n",
        "        extype, exvalue, extrace = sys.exc_info()\n",
        "        traceback.print_exception(extype, exvalue, extrace)\n",
        "    \n",
        "train_set, test_set = split_train_test(\"raw_data/mturk_experiment_2.csv\")\n",
        "\n",
        "print(\"# train, # test: \", len(train_set), len(test_set))\n",
        "\n",
        "headers = [\"sentence_id\", \"text\", \"len_in_words\", \"len_in_char\", \"formality\", \"label\"]\n",
        "\n",
        "with open('input_data/training.csv', 'w') as csvoutput:\n",
        "    writer = csv.writer(csvoutput)\n",
        "    writer.writerow(headers)\n",
        "    for item in train_set:\n",
        "        writer.writerow(item)\n",
        "    \n",
        "with open('input_data/testing.csv', 'w') as csvoutput:\n",
        "    writer = csv.writer(csvoutput)\n",
        "    writer.writerow(headers)\n",
        "    for item in test_set:\n",
        "        writer.writerow(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3830 3202\n",
            "6328 704\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}