{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sherry975/CSC-791-Project/blob/master/Writting%20Styles%20Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3ZYZFJ4p0Gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "# !ls \"/content/drive/My Drive/\"\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/CSC791-NLP/791 NLP project/791_project_repo\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "\n",
        "import sys\n",
        "import traceback\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "import nltk\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt1ZXtD5VSnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#WordtoVec -- GoogleNews\n",
        "import gensim.models.keyedvectors as word2vec\n",
        "model = word2vec.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AneQpO3rVhAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wordList2vec(word_list):\n",
        "    vecs = [model[word] for word in word_list if word in model.vocab]\n",
        "    ret = np.mean(vecs, axis=0)\n",
        "    return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDxW1FIcBP66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def pickle_save(filename, data2pkl):\n",
        "    global pickle_overwrite\n",
        "    if pickle_overwrite == True:\n",
        "        fileObject = open(filename,'wb') \n",
        "        pickle.dump(data2pkl,fileObject)\n",
        "        fileObject.close()\n",
        "\n",
        "        \n",
        "def pickle_load(filename):\n",
        "    fileObject = open(filename,'rb') \n",
        "    data_unpkl = pickle.load(fileObject)\n",
        "    fileObject.close()\n",
        "    return data_unpkl\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6UkD6KhMGXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dictionary(filename):\n",
        "  rows = pickle_load(filename)\n",
        "  tokens = {}\n",
        "  tokens['marked_token'] = []\n",
        "  tokens['sentence_id'] = []\n",
        "  tokens['label'] = []\n",
        "  tokens['token'] = []\n",
        "  for row in rows:\n",
        "    marked_token = row[7]\n",
        "    originate_token = row[6]\n",
        "    sentence_id = row[0]    \n",
        "    label = row[5]\n",
        "    tokens['sentence_id'].append(sentence_id)\n",
        "    tokens['marked_token'].append([re.sub('[^0-9a-zA-Z]+', ' ', str(marked_token)).lower()])\n",
        "    tokens['label'].append(label)\n",
        "    tokens['token'].append([re.sub('[^0-9a-zA-Z]+', ' ', str(originate_token)).lower()])\n",
        "   \n",
        "  return tokens\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJbKMWM_eUVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_#build model\n",
        "\n",
        "def evaluation(X_train, Y_train, X_test, Y_test_true):\n",
        "  from sklearn.linear_model import LogisticRegression\n",
        "  Y_test_sys = []\n",
        "  Y_test_proba = []\n",
        "  model = LogisticRegression(solver='liblinear').fit(X_train, np.ravel(Y_train))\n",
        "  for k in X_test:\n",
        "      Y_test_sys.append(model.predict([k]))\n",
        "      Y_test_proba.append(model.predict_proba([k]))\n",
        "  results = zip(Y_test_sys, Y_test_proba)\n",
        " \n",
        "  from sklearn.metrics import f1_score\n",
        "  from sklearn.metrics import confusion_matrix\n",
        "  from sklearn.metrics import precision_score\n",
        "  from sklearn.metrics import classification_report\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  #f1 score\n",
        "  print('testing performance feature set ')\n",
        "  print('f1 score = ', f1_score(Y_test_true, Y_test_sys, average='weighted'))\n",
        "\n",
        "  #accuracy score\n",
        "\n",
        "  score = accuracy_score(Y_test_true, Y_test_sys)\n",
        "  print(\"accuracy score = \", score)\n",
        "\n",
        "  #precision score\n",
        "  print(\"precision score = \", precision_score(Y_test_true, Y_test_sys, average='weighted'))\n",
        "\n",
        "  #confusion matrix\n",
        "  print(\"confusion matrix = \")\n",
        "  print(confusion_matrix(Y_test_true, Y_test_sys))\n",
        "\n",
        "  print('report : ')\n",
        "  print(classification_report(Y_test_true, Y_test_sys))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FofqER1YK4bu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "training_full_data = {}\n",
        "training_full_data = build_dictionary(\"output_data/training_masked_list.pkl\")\n",
        "#print(Y)\n",
        "testing_full_data = {}\n",
        "testing_full_data = build_dictionary(\"output_data/testing_masked_list.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlNUTsr3V4Ig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#marked tokens\n",
        "vectorized_marked = []\n",
        "\n",
        "i=0\n",
        "for row in training_full_data['marked_token']:\n",
        "  #print(row)\n",
        "  var = ''.join(row)\n",
        "  vec1 = wordList2vec(var.split())\n",
        "  if not np.isnan(vec1).any():\n",
        "    vectorized_marked.append([training_full_data['label'][i], vec1])\n",
        " \n",
        "  i += 1\n",
        "\n",
        "#print(vectorized)\n",
        "X_train_marked = np.array([(data[-1]) for data in vectorized_marked])\n",
        "Y_train_marked = np.array([(data[0]) for data in vectorized_marked])\n",
        "\n",
        "i=0\n",
        "vectorized_marked = []\n",
        "for row in testing_full_data['marked_token']:\n",
        "  #print(row)\n",
        "  var = ''.join(row)\n",
        "  vec1 = wordList2vec(var.split())\n",
        "  if not np.isnan(vec1).any():\n",
        "    vectorized_marked.append([testing_full_data['label'][i], vec1])\n",
        "  i += 1\n",
        "\n",
        "\n",
        "X_test_marked = np.array([data[-1] for data in vectorized_marked])\n",
        "Y_test_marked = np.array([data[0] for data in vectorized_marked])\n",
        "\n",
        "print(len(Y_test_marked))\n",
        "print(len(X_test_marked))\n",
        "print(len(Y_train_marked))\n",
        "print(len(X_train_marked))\n",
        "\n",
        "print(Y_train_marked)\n",
        "evaluation(X_train_marked, Y_train_marked, X_test_marked, Y_test_marked)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2QUZguYlsMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#originated tokens\n",
        "vectorized_originated = []\n",
        "\n",
        "i=0\n",
        "for row in training_full_data['token']:\n",
        "  var = ''.join(row)\n",
        "  vec1 = wordList2vec(var.split())\n",
        "  if not np.isnan(vec1).any():\n",
        "    vectorized_originated.append([training_full_data['label'][i], vec1])\n",
        "    \n",
        "  i += 1\n",
        "\n",
        "#print(vectorized)\n",
        "X_train_originated = np.array([(data[-1]) for data in vectorized_originated])\n",
        "Y_train_originated = np.array([(data[0]) for data in vectorized_originated])\n",
        "#print(Y)\n",
        "\n",
        "i=0\n",
        "vectorized_originated = []\n",
        "for row in testing_full_data['token']:\n",
        "  #print(row)\n",
        "  var = ''.join(row)\n",
        "  vec1 = wordList2vec(var.split())\n",
        "  if not np.isnan(vec1).any():\n",
        "    vectorized_originated.append([testing_full_data['label'][i], vec1])\n",
        "  i += 1\n",
        "\n",
        "\n",
        "X_test_originated = np.array([data[-1] for data in vectorized_originated])\n",
        "Y_test_originated = np.array([data[0] for data in vectorized_originated])\n",
        "\n",
        "evaluation(X_train_originated, Y_train_originated, X_test_originated, Y_test_originated)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji7zBbWCLVg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ngram\n",
        "from nltk import word_tokenize \n",
        "from nltk.util import ngrams\n",
        "from gensim.models import Phrases\n",
        "from gensim.models.phrases import Phraser\n",
        "\n",
        "#training data\n",
        "sentences = []\n",
        "\n",
        "for row in training_full_data['token']:\n",
        "  var = ''.join(row)\n",
        "  sentences.append(var)\n",
        "\n",
        "sentence_stream = [sentence.split(\" \") for sentence in sentences]\n",
        "\n",
        "bigram = Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')\n",
        "bigram_phraser = Phraser(bigram)\n",
        "trigram = Phrases(bigram[sentence_stream], min_count=1, threshold=2, delimiter=b' ')\n",
        "trigram_phraser = Phraser(trigram)\n",
        "\n",
        "\n",
        "vectorized = []\n",
        "vectorized1 = []\n",
        "i = 0\n",
        "#model = word2vec.Word2Vec(bigram_phraser[sentence_stream], size=300, workers=3)\n",
        "\n",
        "for word in sentence_stream:\n",
        "  tokens_ = bigram_phraser[word]\n",
        "  vec1 = wordList2vec(tokens_)\n",
        "  tokens_1 = trigram_phraser[word]\n",
        "  vec2 = wordList2vec(tokens_1)\n",
        "  #print(tokens_1)\n",
        "  if not np.isnan(vec1).any():\n",
        "    vectorized.append([training_full_data['label'][i], vec1])\n",
        "  if not np.isnan(vec2).any():\n",
        "    vectorized1.append([training_full_data['label'][i], vec2])\n",
        "  i += 1\n",
        "\n",
        "X_train_marked = np.array([(data[-1]) for data in vectorized])\n",
        "Y_train_marked = np.array([(data[0]) for data in vectorized])\n",
        "X_train_marked1 = np.array([(data[-1]) for data in vectorized1])\n",
        "Y_train_marked1 = np.array([(data[0]) for data in vectorized1])\n",
        "print(model.most_similar('New_York'))\n",
        "\n",
        "#testing data\n",
        "sentences = []\n",
        "\n",
        "for row in testing_full_data['token']:\n",
        "  var = ''.join(row)\n",
        "  sentences.append(var)\n",
        "\n",
        "sentence_stream = [sentence.split(\" \") for sentence in sentences]\n",
        "\n",
        "bigram = Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')\n",
        "bigram_phraser = Phraser(bigram)\n",
        "trigram = Phrases(bigram[sentence_stream], min_count=1, threshold=2, delimiter=b' ')\n",
        "trigram_phraser = Phraser(trigram)\n",
        "\n",
        "vectorized = []\n",
        "vectorized1 = []\n",
        "i = 0\n",
        "#model = word2vec.Word2Vec(bigram_phraser[sentence_stream], size=300, workers=3)\n",
        "for word in sentence_stream:\n",
        "  tokens_ = bigram_phraser[word]\n",
        "  vec1 = wordList2vec(tokens_)\n",
        "  tokens_1 = trigram_phraser[word]\n",
        "  vec2 = wordList2vec(tokens_1)\n",
        "  #print(tokens_1)\n",
        "  if not np.isnan(vec1).any():\n",
        "    vectorized.append([testing_full_data['label'][i], vec1])\n",
        "  if not np.isnan(vec2).any():\n",
        "    vectorized1.append([testing_full_data['label'][i], vec2])\n",
        "  i += 1\n",
        "\n",
        "X_test_marked = np.array([(data[-1]) for data in vectorized])\n",
        "Y_test_marked = np.array([(data[0]) for data in vectorized])\n",
        "X_test_marked1 = np.array([(data[-1]) for data in vectorized1])\n",
        "Y_test_marked1 = np.array([(data[0]) for data in vectorized1])\n",
        "print(len(Y_test_marked))\n",
        "print(len(X_test_marked))\n",
        "print(len(Y_train_marked))\n",
        "print(len(X_train_marked))\n",
        "\n",
        "print(Y_train_marked)\n",
        "evaluation(X_train_marked, Y_train_marked, X_test_marked, Y_test_marked)\n",
        "evaluation(X_train_marked1, Y_train_marked1, X_test_marked1, Y_test_marked1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}